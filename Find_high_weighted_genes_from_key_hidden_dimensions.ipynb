{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial import distance\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def Euclidean_dist(A, B):\n",
    "    C = A - B\n",
    "    return sum(map(sum, C * C)) ** 0.5\n",
    "\n",
    "\n",
    "def MAE(A, B):  ## Mean Absolute Error\n",
    "    C = A - B\n",
    "    return sum(map(sum, C * C)) / (C.shape[0] * C.shape[1])\n",
    "\n",
    "\n",
    "def random_split_train_test(X0, training_dictionary_fraction, seed, dictionary_size=0.5, biased_training=0.):\n",
    "    training_dictionary_size = max(int(training_dictionary_fraction * X0.shape[1]), 5)\n",
    "    if dictionary_size < 1:\n",
    "        dictionary_size = dictionary_size * training_dictionary_size\n",
    "    dictionary_size = int(dictionary_size)\n",
    "    xi = np.zeros(X0.shape[1], dtype=np.bool)\n",
    "    if biased_training > 0:\n",
    "        np.random.seed(seed)\n",
    "        i = np.random.randint(len(xi))\n",
    "        dist = distance.cdist([X0[:, i]], X0.T, 'correlation')[0]\n",
    "        didx = np.argsort(dist)[1:int(biased_training * training_dictionary_size) + 1]\n",
    "    else:\n",
    "        didx = []\n",
    "    xi[didx] = True\n",
    "    if biased_training < 1:\n",
    "        remaining_idx = np.setdiff1d(range(len(xi)), didx)\n",
    "        np.random.seed(seed)\n",
    "        xi[np.random.choice(remaining_idx, training_dictionary_size - xi.sum(), replace=False)] = True\n",
    "    xa = X0[:, xi]\n",
    "    xb = X0[:, np.invert(xi)]\n",
    "    return xa, xb\n",
    "\n",
    "\n",
    "def compare_results(A, B):\n",
    "    results = list((1 - distance.correlation(A.flatten(), B.flatten())))\n",
    "    results += list(Euclidean_dist(A, B))\n",
    "    results += list(MAE(A, B))\n",
    "    return results\n",
    "\n",
    "\n",
    "seed_all = {\"GSE71858\": [272, 781, 692, 219, 292],  #\n",
    "            \"GSE60361\": [283, 446, 562, 114, 739],  #\n",
    "            \"GSE62270\": [629, 685, 953, 595, 378],  #\n",
    "            \"GSE48968\": [623, 19, 621, 802, 557],  #\n",
    "            \"GSE52529\": [550, 939, 76, 260, 328],  #\n",
    "            \"GSE77564\": [475, 649, 316, 639, 741],\n",
    "            \"GSE78779\": [152, 866, 808, 796, 184],  #\n",
    "            \"GSE10247\": [702, 217, 944, 338, 701],  #\n",
    "            \"GSE69405\": [317, 470, 798, 283, 695],\n",
    "            \"GSE45235\": [282, 713, 521, 717, 517],  #\n",
    "            \"GSE25038\": [480, 402, 413, 64, 574],\n",
    "            \"mass_cytomatry\": [943, 800, 175, 486, 749]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(1)\n",
    "\n",
    "# Hyper Parameters\n",
    "LR = 0.0001  # learning rate\n",
    "Dropout_rate = 0.5\n",
    "# GSE Data\n",
    "data_path = \"./Original_data/GSE78779.npy\"\n",
    "X = np.load(data_path)\n",
    "training_dictionary_fraction = 0.05\n",
    "genes, samples = X.shape\n",
    "seeds = seed_all['GSE78779']\n",
    "############################# Define architectures ##################################\n",
    "# tf placeholder\n",
    "tf_x = tf.placeholder(tf.float32, [None, genes])  # value in the range of (0, 1)\n",
    "\n",
    "# encoder\n",
    "# Dn0 = tf.layers.dropout(tf_x, rate=Dropout_rate, training=True)\n",
    "en0 = tf.layers.dense(tf_x, 1280, tf.nn.leaky_relu)\n",
    "en1 = tf.layers.dense(en0, 640, tf.nn.leaky_relu)\n",
    "en2 = tf.layers.dense(en1, 256, tf.nn.leaky_relu)\n",
    "\n",
    "encoded = tf.layers.dense(en2, 10)\n",
    "\n",
    "# decoder\n",
    "de0 = tf.layers.dense(encoded, 256, tf.nn.leaky_relu)\n",
    "de1 = tf.layers.dense(de0, 640, tf.nn.leaky_relu)\n",
    "de2 = tf.layers.dense(de1, 1280, tf.nn.leaky_relu)\n",
    "\n",
    "decoded = tf.layers.dense(de2, genes, tf.nn.leaky_relu)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(labels=tf_x, predictions=decoded)\n",
    "train = tf.train.AdamOptimizer(LR).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 7.0795030e-01, 1.0996994e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       ...,\n",
       "       [1.4634321e+00, 0.0000000e+00, 1.7354449e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 5.6049053e-01],\n",
       "       [3.4370648e+02, 1.2464050e+03, 1.3840015e+03, ..., 3.7035251e+02,\n",
       "        3.4753925e+02, 3.2839333e+00],\n",
       "       [2.3643041e+02, 0.0000000e+00, 2.0549943e+02, ..., 5.1149534e+01,\n",
       "        0.0000000e+00, 9.8031205e+01]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_path = \"./Original_data/GSE48968.npy\"\n",
    "X = np.load(data_path)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[152, 866, 808, 796, 184]\n",
      "(22814, 96)\n",
      "(22814, 5)\n",
      "(22814, 91)\n",
      "[1. 1. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "############################# Running ##################################\n",
    "Results = {}\n",
    "# seeds = random.sample(range(0, 1000), 5)\n",
    "# seeds = [283, 446, 562, 114, 739]\n",
    "\n",
    "print(seeds)\n",
    "for i in range(2):\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    X_train, X_test = random_split_train_test(X, training_dictionary_fraction, seed=seeds[i])\n",
    "    #print(xi)\n",
    "    #np.savetxt(\"GSE60361_Xi.csv\", xi, delimiter=',')\n",
    "    print(X.shape)  #\n",
    "    print(X_train.shape)  #\n",
    "    print(X_test.shape)  #\n",
    "    print(X_train[0, 0:10])\n",
    "    X_train = np.transpose(X_train)\n",
    "    X_test = np.transpose(X_test)\n",
    "\n",
    "    for step in range(500):\n",
    "        b_x = X_train\n",
    "        _, encoded_, decoded_, loss_ = sess.run([train, encoded, decoded, loss], {tf_x: b_x})\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            # print('------------------Step: %d' % step + '---------------')\n",
    "            # print('train loss: %.4f' % loss_)\n",
    "            # plotting decoded image (second row)\n",
    "            decoded_data_train = sess.run(decoded, {tf_x: b_x})\n",
    "            # train_p = (1 - distance.correlation(X_train.flatten(), decoded_data_train.flatten()))\n",
    "            train_pp = pearsonr(X_train.flatten(), decoded_data_train.flatten())[0]\n",
    "            train_ED = Euclidean_dist(X_train, decoded_data_train)\n",
    "            train_MAE = MAE(X_train, decoded_data_train)\n",
    "            # print('train Pearson: %.4f' % train_p)\n",
    "            # print('train Pearson_: %.4f' % train_pp)\n",
    "            # print('train Euclidean_dist: %e' % train_ED)\n",
    "            # print('train MAE: %.4f' % train_MAE)\n",
    "\n",
    "            encod = sess.run(encoded, {tf_x: b_x})\n",
    "            # print(encod.shape)\n",
    "            # print('------------------Test---------------')\n",
    "            decoded_data_testing = sess.run(decoded, {tf_x: X_test})\n",
    "            encoded_data = sess.run(encoded, {tf_x: X_test})\n",
    "            # test_p = (1 - distance.correlation(X_test.flatten(), decoded_data.flatten()))\n",
    "            test_pp = pearsonr(X_test.flatten(), decoded_data_testing.flatten())[0]\n",
    "            test_ED = Euclidean_dist(X_test, decoded_data_testing)\n",
    "            test_MAE = MAE(X_test, decoded_data_testing)\n",
    "            # print('test Pearson: %.4f' % test_p)\n",
    "            # print('test Pearson_: %.4f' % test_pp)\n",
    "            # print('test Euclidean_dist: %e' % test_ED)\n",
    "            # print('test MAE: %.4f' % test_MAE)\n",
    "            # print('----------------------------------------')\n",
    "    #       Result = compare_results(X_test, decoded_data)\n",
    "    #       print(Result)\n",
    "    decoded_data_testing = sess.run(decoded, {tf_x: X_test})\n",
    "\n",
    "    print(decoded_data_testing.shape)\n",
    "\n",
    "    result_train = 'DeepAE4 (training)_' + str(i)\n",
    "    result_test = 'DeepAE4 (testing )_' + str(i)\n",
    "    Results[result_train] = [train_pp, train_ED, train_MAE]\n",
    "    Results[result_test] = [test_pp, test_ED, test_MAE]\n",
    "    print('----------------End Iteration: %d' % i + '------------------------')\n",
    "\n",
    "print(data_path)\n",
    "for k, v in sorted(Results.items()):\n",
    "    print('\\t'.join([k] + [str(x) for x in v]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(19972, 1280) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/bias:0' shape=(1280,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_1/kernel:0' shape=(1280, 640) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(640,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_2/kernel:0' shape=(640, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_2/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_3/kernel:0' shape=(256, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(10,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_4/kernel:0' shape=(10, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_4/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_5/kernel:0' shape=(256, 640) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_5/bias:0' shape=(640,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_6/kernel:0' shape=(640, 1280) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_6/bias:0' shape=(1280,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_7/kernel:0' shape=(1280, 19972) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_7/bias:0' shape=(19972,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1=tf.get_default_graph().get_tensor_by_name('dense/kernel:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Weights = sess.run(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19972, 1280)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n"
     ]
    }
   ],
   "source": [
    "out_w1 = sess.run(tf.get_default_graph().get_tensor_by_name('dense_4/kernel:0'))\n",
    "out_b1 = sess.run(tf.get_default_graph().get_tensor_by_name('dense_4/bias:0'))\n",
    "chl1 = np.dot(out_w1.T, chl) + out_b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_w2 = sess.run(tf.get_default_graph().get_tensor_by_name('dense_5/kernel:0'))\n",
    "out_b2 = sess.run(tf.get_default_graph().get_tensor_by_name('dense_5/bias:0'))\n",
    "chl2 = np.dot(out_w2.T, chl1) + out_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_w3 = sess.run(tf.get_default_graph().get_tensor_by_name('dense_6/kernel:0'))\n",
    "out_b3 = sess.run(tf.get_default_graph().get_tensor_by_name('dense_6/bias:0'))\n",
    "chl3 = np.dot(out_w3.T, chl2) + out_b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_w4 = sess.run(tf.get_default_graph().get_tensor_by_name('dense_7/kernel:0'))\n",
    "out_b4 = sess.run(tf.get_default_graph().get_tensor_by_name('dense_7/bias:0'))\n",
    "chl4 = np.dot(out_w4.T, chl3) + out_b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256,)\n",
      "(256, 640)\n",
      "(640,)\n",
      "(640,)\n",
      "(1280,)\n",
      "(19972,)\n"
     ]
    }
   ],
   "source": [
    "print(chl1.shape)\n",
    "print(out_w2.shape)\n",
    "print(out_b2.shape)\n",
    "print(chl2.shape)\n",
    "print(chl3.shape)\n",
    "print(chl4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1097.2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "19972*0.1\n",
    "6789*0.1\n",
    "10972*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-468426a71ee6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mchl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mchl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mout_w1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dense_4/kernel:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mout_b1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dense_4/bias:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mchl1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_w1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchl\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mout_b1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sess' is not defined"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import csv\n",
    "top = []\n",
    "for i in range(10):\n",
    "    chl = np.zeros((10,), dtype=np.int)\n",
    "    chl[i] = 1\n",
    "    out_w1 = sess.run(tf.get_default_graph().get_tensor_by_name('dense_4/kernel:0'))\n",
    "    out_b1 = sess.run(tf.get_default_graph().get_tensor_by_name('dense_4/bias:0'))\n",
    "    chl1 = np.dot(out_w1.T, chl) + out_b1\n",
    "    out_w2 = sess.run(tf.get_default_graph().get_tensor_by_name('dense_5/kernel:0'))\n",
    "    out_b2 = sess.run(tf.get_default_graph().get_tensor_by_name('dense_5/bias:0'))\n",
    "    chl2 = np.dot(out_w2.T, chl1) + out_b2\n",
    "    out_w3 = sess.run(tf.get_default_graph().get_tensor_by_name('dense_6/kernel:0'))\n",
    "    out_b3 = sess.run(tf.get_default_graph().get_tensor_by_name('dense_6/bias:0'))\n",
    "    chl3 = np.dot(out_w3.T, chl2) + out_b3\n",
    "    out_w4 = sess.run(tf.get_default_graph().get_tensor_by_name('dense_7/kernel:0'))\n",
    "    out_b4 = sess.run(tf.get_default_graph().get_tensor_by_name('dense_7/bias:0'))\n",
    "    chl4 = np.dot(out_w4.T, chl3) + out_b4\n",
    "    top10 = heapq.nlargest(22814, range(len(chl4)), chl4.take)\n",
    "    top = np.hstack((top, top10))\n",
    "    \n",
    "np.savetxt(\"GSE78779_top.csv\", top, delimiter=',')\n",
    "print(top.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
